services:
  vllm-api:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "127.0.0.1:8000:8000"
    ipc: host
    command: --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8 --dtype bfloat16 --max-model-len 12432 --gpu-memory-utilization 0.95 --cpu-offload-gb 5 --enforce-eager --max-num-seqs 10
  backend:
    image: quockfamu/tokushukai-main:backend
    ports:
      - "127.0.0.1:5000:5000"
    depends_on:
      - vllm-api
  frontend:
    image: quockfamu/tokushukai-main:frontend
    ports:
      - "127.0.0.1:8501:8501"
    depends_on:
      - backend
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "127.0.0.1:9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "127.0.0.1::3000"
    depends_on:
      - prometheus